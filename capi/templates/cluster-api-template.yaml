apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: {{ .Values.cluster.name }}
  namespace: default
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/16
    serviceDomain: cluster.local
    services:
      cidrBlocks:
        - 10.128.0.0/12
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: {{ .Values.cluster.name }}-control-plane
    namespace: default
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
    kind: DockerCluster
    name: {{ .Values.cluster.name }}
    namespace: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerCluster
metadata:
  name: {{ .Values.cluster.name }}
  namespace: default
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: {{ .Values.cluster.name }}-control-plane
  namespace: default
spec:
  template:
    spec:
      extraMounts:
        - containerPath: /var/run/docker.sock
          hostPath: /var/run/docker.sock
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
kind: KubeadmControlPlane
metadata:
  name: {{ .Values.cluster.name }}-control-plane
  namespace: default
spec:
  kubeadmConfigSpec:
    clusterConfiguration:
      apiServer:
        certSANs:
          - localhost
          - 127.0.0.1
      controllerManager:
        extraArgs:
          enable-hostpath-provisioner: "true"
    initConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: cgroupfs
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
    joinConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cgroup-driver: cgroupfs
          eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
  machineTemplate:
    infrastructureRef:
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
      kind: DockerMachineTemplate
      name: {{ .Values.cluster.name }}-control-plane
      namespace: default
  replicas: {{ .Values.cluster.masterNodes }}
  version: {{ .Values.cluster.version }}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: DockerMachineTemplate
metadata:
  name: {{ .Values.cluster.name }}-md-0
  namespace: default
spec:
  template:
    spec: {}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
kind: KubeadmConfigTemplate
metadata:
  name: {{ .Values.cluster.name }}-md-0
  namespace: default
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cgroup-driver: cgroupfs
            eviction-hard: nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: {{ .Values.cluster.name }}-md-0
  namespace: default
spec:
  clusterName: {{ .Values.cluster.name }}
  replicas: {{ .Values.cluster.workerNodes }}
  selector:
    matchLabels: null
  template:
    spec:
      bootstrap:
        configRef:
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
          name: {{ .Values.cluster.name }}-md-0
          namespace: default
      clusterName: {{ .Values.cluster.name }}
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: DockerMachineTemplate
        name: {{ .Values.cluster.name }}-md-0
        namespace: default
      version: {{ .Values.cluster.version }}
---
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: HookSucceeded
  name: capi-setup-{{ .Values.cluster.name }}-job
  namespace: argocd
spec:
  template:
    spec:
      containers:
        - image: quay.io/redhatworkshops/capi-tools:latest
          command:
            - /bin/bash
            - -c
            - |
              echo ""
              echo -n "Waiting for CAPI Cluster $CAPI_CLUSTER_NAME CP to be ready..."
              sleep $WAIT
              COUNTER=0
              # Wait until number of CP nodes are staged
              until [[ $(kubectl get kubeadmcontrolplane $CAPI_CLUSTER_NAME-control-plane -n default -o jsonpath="{.status.updatedReplicas}") -eq $CAPI_CP_NODES ]]
              do
                sleep $WAIT
                COUNTER=$(( COUNTER + 1))
                if [[ $COUNTER -gt $CAPI_WAIT_TIMEOUT ]]; then
                  echo "TIMEOUT REACHED"
                  exit 13
                fi
              done
              # build config
              /usr/local/bin/create-kconfig.sh
              sleep $WAIT
              # get the kubeconfig of the capi cluster
              clusterctl get kubeconfig $CAPI_CLUSTER_NAME > $HOME/$CAPI_CLUSTER_NAME.kubeconfig
              sleep $WAIT
              # apply CNI
              kubectl --kubeconfig=$HOME/$CAPI_CLUSTER_NAME.kubeconfig apply -f $CAPI_CNI_YAML_URL
              sleep $WAIT
          imagePullPolicy: Always
          name: capi-setup-container
          env:
          - name: WAIT
            value: "{{ .Values.cluster.capiSetupWait }}"
          - name: CAPI_CLUSTER_NAME
            value: "{{ .Values.cluster.name }}"
          - name: CAPI_CP_NODES
            value: "{{ .Values.cluster.masterNodes }}"
          - name: CAPI_CNI_YAML_URL
            value: "{{ .Values.cluster.capiClusterCNI }}"
          - name: CAPI_WAIT_TIMEOUT
            value: "{{ .Values.cluster.capiSetupTimeout }}"
      dnsPolicy: ClusterFirst
      restartPolicy: OnFailure
      serviceAccountName: argocd-application-controller
      terminationGracePeriodSeconds: 90
